{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB2cisc3140midterm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chalshaff12/sharing-repo/blob/master/NB2cisc3140midterm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONqTy_Aa0ZW2",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraping\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "by Michal Shaffer\n",
        "\n",
        "**Using web scraping to extract and manipulate data from the goodreads 2018 Fiction choice awards list**\n",
        "\n",
        "\n",
        "---\n",
        "Goodreads.com has a yearly [choice awards event](https://www.goodreads.com/choiceawards/best-fiction-books-2018) where users vote on their favorite new books. After the event ends, goodreads publishes the list of nominees per genre along with the total vote count that each nominee received. .\n",
        "\n",
        "#####For this project the following tasks will be completed:\n",
        "\n",
        "\n",
        "1.   Set up the webpage url and prepare to get the data\n",
        "2.   Scrape the raw html data from the webpage\n",
        "3.   Parse through and extract return data\n",
        "4.   Manipulate extracted data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd-we8UG13z0",
        "colab_type": "text"
      },
      "source": [
        "##1) Initial set-up and preparation\n",
        "\n",
        "In order to get any data from our page, we first have to do some initial setup procedures.\n",
        "\n",
        "First, we'll need to get the html data, so we'll need to utilize the [`requests`](https://2.python-requests.org/en/master/) library. Let's import that first:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt6YcXFJ-dhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1zeXv9Z2r-k",
        "colab_type": "text"
      },
      "source": [
        "##2) Scraping the data from the page\n",
        "\n",
        "Next, let's set up our page information and get the html dump of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzWAv44A-eJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genre = 'fiction' #we are looking at the fiction list for this project\n",
        "url = f'https://www.goodreads.com/choiceawards/best-{genre}-books-2018'\n",
        "page_html = requests.get(url).text #this gets the html text data and saves it into our variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwN766Vs24Wz",
        "colab_type": "text"
      },
      "source": [
        "##3a) Parsing through our data\n",
        "\n",
        "Html is difficult to parse through using python, so we need the help of another library. One of the very useful libraries is [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). (Click the link to view the documentation for that library.) \n",
        "\n",
        "Let's import that library first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVK5JuL2-5-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUZxPMX3uT7",
        "colab_type": "text"
      },
      "source": [
        "Now, let's utilize the new library by parsing the html data into a more usable form:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipu_NBpJ6IcR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZyvxJVK3vF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup_data = BeautifulSoup(page_html, 'lxml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpU8UBuL4C6Q",
        "colab_type": "text"
      },
      "source": [
        "Great! You can print out all the newly-formatted html with\n",
        "\n",
        "> `print(soup_data)`\n",
        "\n",
        "but I don't want to clutter the page, so let's leave that out for now.\n",
        "\n",
        "After looking through our `soup_data`, I found my way to the area of the page where the list of book nominees are displayed and made note of the html tags and class names.\n",
        "\n",
        "![](https://i.imgur.com/9uX1YdN.png)\n",
        "\n",
        "Using this information, let's navigate to that section now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCOYffzh4CgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poll_contents = soup_data.find('div', {'pollContents'}) #first get to the main block of content\n",
        "poll_item = poll_contents.find_all('div',{'inlineblock pollAnswer resultShown'}) #from there, get to the list of books"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2HHTLSA7IP2",
        "colab_type": "text"
      },
      "source": [
        "##3b) Extracting our data\n",
        "\n",
        "Great! Now I want to go through each 'pollAnswer' to get the data for each nominated book. \n",
        "However, I can already see that the data is not in the format that I want to save it in. So before we run through it, let's make a few quick functions to help us slice and dice some of the string data we'll be getting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kNNNjvB9k8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gets the characters between a given start char or string and end char or string\n",
        "def between(string, start, end):\n",
        "  start_pos = string.find(start) #find the start position of the inputted start\n",
        "  end_pos = string.rfind(end) #find the end position of the inputted end\n",
        "  new_start_pos = start_pos + len(start) #set the new starting position to the end of the inputted start\n",
        "  return string[new_start_pos:end_pos] #Return everything between the two given positions.\n",
        "\n",
        "#gets the characters before a given end char or string\n",
        "def before(string, end):\n",
        "  end_pos = string.find(end) #find the end position of the inputted end\n",
        "  return string[0:end_pos] #return everything before the given position.\n",
        "\n",
        "#gets the characters after a given end char or string\n",
        "def after(string, start):\n",
        "  start_pos = string.rfind(start) #find the start position of the inputted start\n",
        "  new_start_pos = start_pos + len(start) #set the new starting position to the end of the inputted start\n",
        "  return string[new_start_pos:] #return everything after the given position."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgCib4FZ9nbT",
        "colab_type": "text"
      },
      "source": [
        "Great! Now, let's go through each book nominee item on the page and extract the data we want. \n",
        "\n",
        "I will be putting our extracted data into a nested list which will be easy to manipulate later and very easy to read.\n",
        "\n",
        "Our list items will each include the book title, author, number of votes, and image url. It will look something like this:\n",
        "\n",
        ">`[['title', 'author', votes, 'image url'], ['Harry Potter number 1', 'JK Rowling', 999999, 'http://www.someurl.com/image.jpg'], ['The Way of Kings','Brandon Sanderson', 234523, 'anotherimage.com/picture.png']]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilGKX3nP-_sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create an empty list for our data\n",
        "book_list = []\n",
        "\n",
        "#begin looping through each poll item\n",
        "for book in poll_item:\n",
        "  title_tag = book.find('a', {'pollAnswer__bookLink'}) #navigate to the html tag holding the title information\n",
        "  title = between(str(title_tag), 'title=\\\"', ' by '), #extract the book title with the between function \n",
        "  author = between(str(title_tag), ' by ', '\\\" src'),  #extract the author with the between function\n",
        "  votes = book.find('strong',{'uitext result'}).text   #get the number of votes from its location\n",
        "  vote = before(votes, \"votes\").strip()   #extract the actual number of votes with the before function and strip the whitespace\n",
        "  image = between(str(title_tag), 'src=\\\"', '\\\" title'), #extract the image url with between function\n",
        "  #add the data as a list to our book list\n",
        "  book_list.append([title[0], author[0], int(vote.replace(',','')), image[0]]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6htMkvm_BzM",
        "colab_type": "text"
      },
      "source": [
        "Cool! Let's have a look at our list of extracted data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFVTxT18_Fqu",
        "colab_type": "code",
        "outputId": "886d5037-695a-4dc1-be7f-e3cbcb8e77b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "print(\"List of book data:\")\n",
        "for book in book_list:\n",
        "  print (book)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of book data:\n",
            "['Still Me', 'Jojo Moyes', 55300, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1502818159l/35791968.jpg']\n",
            "['An American Marriage', 'Tayari Jones', 41826, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1491493625l/33590210._SY475_.jpg']\n",
            "['Us Against You', 'Fredrik Backman', 38981, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1516019348l/36373463._SY475_.jpg']\n",
            "['An Absolutely Remarkable Thing', 'Hank Green', 24363, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1520448919l/38186611.jpg']\n",
            "['Killing Commendatore', 'Haruki Murakami', 23695, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1527854255l/38820047.jpg']\n",
            "['There There', 'Tommy Orange', 18614, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1512071034l/36692478.jpg']\n",
            "['All We Ever Wanted', 'Emily Giffin', 17490, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1512578780l/36723031.jpg']\n",
            "['Every Note Played', 'Lisa Genova', 15427, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1521364066l/36082326._SY475_.jpg']\n",
            "['The Female Persuasion', 'Meg Wolitzer', 7343, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1503001734l/35480518._SY475_.jpg']\n",
            "['A Spark of Light', 'Jodi Picoult', 5554, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1524516474l/39072220.jpg']\n",
            "['My Year of Rest and Relaxation', 'Ottessa Moshfegh', 5045, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1513259517l/36203391._SX318_.jpg']\n",
            "['Everything Here Is Beautiful', 'Mira T. Lee', 4405, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1492886846l/34262106.jpg']\n",
            "[\"You Think It, I'll Say It\", 'Curtis Sittenfeld', 3126, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1509832627l/35952871.jpg']\n",
            "['How to Walk Away', 'Katherine Center', 1973, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1507579931l/36249638.jpg']\n",
            "['The Book of Essie', 'Meghan MacLean Weir', 1641, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1515873936l/36723245.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-OeJPC4EjQq",
        "colab_type": "text"
      },
      "source": [
        "Awesome, we have a usable list of nicely formatted data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGne4kYO_H5K",
        "colab_type": "text"
      },
      "source": [
        "##4) Manipulating our data\n",
        "\n",
        "Let's try some data manipulation.\n",
        "\n",
        "I'll show you how to get the total amount of votes for all the books together, and the average amount of votes per book.\n",
        "\n",
        "Let's first seperate our votes data into it's own list using `list comprehension`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COhxAgTZ_1AE",
        "colab_type": "code",
        "outputId": "4dfbf9c7-3c79-4462-f8de-fe0cb7936f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "all_votes = [book[2] for book in book_list] \n",
        "print(\"List of votes:\" , all_votes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of votes: [55300, 41826, 38981, 24363, 23695, 18614, 17490, 15427, 7343, 5554, 5045, 4405, 3126, 1973, 1641]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uah7F5apAHC6",
        "colab_type": "text"
      },
      "source": [
        "Great! Now we can easily get the calculations we want using some simple python tools:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRtJRt6Y_7Ad",
        "colab_type": "code",
        "outputId": "19520635-55b4-4ef6-b0ef-0d9e8868bdb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_votes = sum(all_votes) #sum() function sums up our list\n",
        "avg_votes = total_votes/len(all_votes) #divide the sum by the length of the list to get the average\n",
        "print(\"Total votes:\" , total_votes, \". Average vote: \", avg_votes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total votes: 264783 . Average vote:  17652.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niFQQuBkAuEh",
        "colab_type": "text"
      },
      "source": [
        "That was easy!\n",
        "\n",
        "Let's try one more thing with our scraped data.\n",
        "\n",
        "The book nominees show on the website in the order of highest votes to lowest, showing their ranking in the choice awards competition. Let's try to switch the order, and see the lowest voted books first!\n",
        "\n",
        "To accomplish this, we'll use python's `sorted()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLEm0CPu-6Q1",
        "colab_type": "code",
        "outputId": "f8fcae47-2d6e-4c3e-f07d-2b08ec4ea3b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "#sorted(book_list) sorts the list in ascending order\n",
        "#key tells the list what to sort by\n",
        "#our lambda function sets the key as the second index of each item (or sublist) of the list\n",
        "ascending_book_list = sorted(book_list, key = lambda book: book[2])\n",
        "\n",
        "print(\"Book list by ascending votes\")\n",
        "for book in ascending_book_list:\n",
        "  print(book)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Book list by ascending votes\n",
            "['The Book of Essie', 'Meghan MacLean Weir', 1641, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1515873936l/36723245.jpg']\n",
            "['How to Walk Away', 'Katherine Center', 1973, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1507579931l/36249638.jpg']\n",
            "[\"You Think It, I'll Say It\", 'Curtis Sittenfeld', 3126, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1509832627l/35952871.jpg']\n",
            "['Everything Here Is Beautiful', 'Mira T. Lee', 4405, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1492886846l/34262106.jpg']\n",
            "['My Year of Rest and Relaxation', 'Ottessa Moshfegh', 5045, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1513259517l/36203391._SX318_.jpg']\n",
            "['A Spark of Light', 'Jodi Picoult', 5554, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1524516474l/39072220.jpg']\n",
            "['The Female Persuasion', 'Meg Wolitzer', 7343, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1503001734l/35480518._SY475_.jpg']\n",
            "['Every Note Played', 'Lisa Genova', 15427, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1521364066l/36082326._SY475_.jpg']\n",
            "['All We Ever Wanted', 'Emily Giffin', 17490, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1512578780l/36723031.jpg']\n",
            "['There There', 'Tommy Orange', 18614, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1512071034l/36692478.jpg']\n",
            "['Killing Commendatore', 'Haruki Murakami', 23695, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1527854255l/38820047.jpg']\n",
            "['An Absolutely Remarkable Thing', 'Hank Green', 24363, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1520448919l/38186611.jpg']\n",
            "['Us Against You', 'Fredrik Backman', 38981, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1516019348l/36373463._SY475_.jpg']\n",
            "['An American Marriage', 'Tayari Jones', 41826, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1491493625l/33590210._SY475_.jpg']\n",
            "['Still Me', 'Jojo Moyes', 55300, 'https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1502818159l/35791968.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcJaF2DLTJf",
        "colab_type": "text"
      },
      "source": [
        "## Signing off\n",
        "\n",
        "Web scraping is a powerful, if controversial tool. It allows us to get information from websites that may not offer API access to the data we want and manipulate it to suit our needs.\n",
        "\n",
        "Thank you\n"
      ]
    }
  ]
}